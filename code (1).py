# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cb0yp-txT5WuwrbWOWeYPCsecxQD12wF

Step 1: Cloning the GitHub Repo and Installing Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/yanivnik/tuning_cv_models_with_rl_torch.git
# %cd tuning_cv_models_with_rl_torch
!pip install torch torchvision transformers tensorboard

"""Step 2: Importing Libraries


"""

import torch
import torchvision.transforms as transforms
from transformers import ViTModel, ViTConfig
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from skimage.metrics import structural_similarity as ssim
from skimage.color import rgb2lab
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset
import torch.optim as optim

"""Step 3: Defining Image Transforms

"""

#convert images to grayscale (input)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Lambda(lambda x: torch.stack([x.mean(0)] * 3))  # Grayscale as input
])

#keep images as RGB (target output)
target_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()  # RGB as target
])

"""Step 4: Downloading Sample Image Dataset - Imagenette (Subsection of ImageNet)


"""

!mkdir imagenet_sample
!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz -P imagenet_sample
!tar -xzf imagenet_sample/imagenette2-160.tgz -C imagenet_sample

#Number of images in the dataset: 13397

"""Step 5: Seting Up Dataset and DataLoader


"""

from torchvision.datasets import ImageFolder

class ColorizationDataset(Dataset):
    def __init__(self, root, transform=None, target_transform=None):
        self.dataset = ImageFolder(root=root)
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, _ = self.dataset[idx]
        grayscale = self.transform(image) if self.transform else image
        rgb = self.target_transform(image) if self.target_transform else image
        return grayscale, rgb

colorization_data = ColorizationDataset(
    root="imagenet_sample/imagenette2-160",
    transform=transform,
    target_transform=target_transform
)
dataloader = DataLoader(colorization_data, batch_size=4, shuffle=True)

"""Step 6: Defining Colorization Model"""

import torch.nn as nn
from transformers import ViTModel

class ColorizationModel(nn.Module):
    def __init__(self):
        super(ColorizationModel, self).__init__()
        # Initialize a ViT model as the encoder (simpler but similar model the one describe on the paper)
        self.encoder = ViTModel.from_pretrained("google/vit-base-patch16-224-in21k")

        # Define a simple decoder to output RGB channels
        self.decoder = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Linear(512, 3 * 224 * 224)  # Output RGB channels for 224x224 image
        )

    def forward(self, x):
        features = self.encoder(x).last_hidden_state  # Extract features
        colorized_image = self.decoder(features[:, 0, :])  # Decode to RGB channels
        colorized_image = colorized_image.view(-1, 3, 224, 224)  # Reshape to (batch, channels, height, width)
        return colorized_image

model = ColorizationModel().to("cuda" if torch.cuda.is_available() else "cpu")
criterion = nn.MSELoss()  # Mean squared error Loss
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

"""Step 7: Pretraining the Model

"""

num_epochs = 5  # Set the number of epochs
device = "cuda" if torch.cuda.is_available() else "cpu"

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for grayscale, rgb in dataloader:
        grayscale, rgb = grayscale.to(device), rgb.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(grayscale)
        loss = criterion(outputs, rgb)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Track the loss
        running_loss += loss.item()

    # Print average loss per epoch
    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}")

# Visualize colorization results before fine-tuning
model.eval()
test_grayscale, test_rgb = next(iter(dataloader))
test_grayscale = test_grayscale.to(device)
with torch.no_grad():
    pre_finetune_output = model(test_grayscale).cpu().clamp(0, 1)

num_images = min(4, test_grayscale.size(0))
for i in range(num_images):
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))
    axs[0].imshow(test_grayscale[i].permute(1, 2, 0).cpu(), cmap="gray")
    axs[0].set_title("Grayscale Input")
    axs[1].imshow(test_rgb[i].permute(1, 2, 0).cpu())
    axs[1].set_title("Expected Color Output")
    axs[2].imshow(pre_finetune_output[i].permute(1, 2, 0))
    axs[2].set_title("Pre-Fine-Tuning Output")
    for ax in axs:
        ax.axis("off")
    plt.show()

"""Step 8: Defining Vividness and Hue Entropy Reward Functions


"""

def calculate_colorfulness_reward(output_image):
    lab_image = rgb2lab(output_image.permute(1, 2, 0).detach().cpu().numpy())
    a_channel, b_channel = lab_image[:, :, 1], lab_image[:, :, 2]

    vivid_mask = (a_channel ** 2 + b_channel ** 2) > 10
    vivid_fraction = vivid_mask.mean()

    hue_values = np.arctan2(b_channel, a_channel)
    hist, _ = np.histogram(hue_values, bins=7, range=(-np.pi, np.pi), density=True)
    hue_entropy = entropy(hist + 1e-6)  #epsilon for numerical stability

    # Combine the two terms
    reward = vivid_fraction * hue_entropy
    return reward

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    running_reward = 0.0

    for grayscale, rgb in dataloader:
        grayscale, rgb = grayscale.to(device), rgb.to(device)


        optimizer.zero_grad()


        outputs = model(grayscale)
        mse_loss = criterion(outputs, rgb)  #original MSE loss
        batch_reward = torch.tensor([calculate_colorfulness_reward(output) for output in outputs]).mean().to(device)
        total_loss = mse_loss - 0.1 * batch_reward  #adjust weighting as necessary
        total_loss.backward()
        optimizer.step()
        running_loss += mse_loss.item()
        running_reward += batch_reward.item()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}, Reward: {running_reward / len(dataloader):.4f}")

def display_comparison(grayscale, ground_truth, pretrained_output, finetuned_output, num_images=4):
    for i in range(num_images):
        fig, axs = plt.subplots(1, 4, figsize=(20, 5))

        #grayscale Input
        axs[0].imshow(grayscale[i].permute(1, 2, 0).cpu(), cmap="gray")
        axs[0].set_title("Grayscale Input")

        #ground Truth
        axs[1].imshow(ground_truth[i].permute(1, 2, 0).cpu())
        axs[1].set_title("Ground Truth RGB")

        #gretrained Model Output
        axs[2].imshow(pretrained_output[i].permute(1, 2, 0).clamp(0, 1))
        axs[2].set_title("Pretrained Output")

        #fine-tuned Model Output
        axs[3].imshow(finetuned_output[i].permute(1, 2, 0).clamp(0, 1))
        axs[3].set_title("Fine-Tuned Output")

        for ax in axs:
            ax.axis("off")
        plt.show()

test_grayscale, test_rgb = next(iter(dataloader))
test_grayscale = test_grayscale.to(device)

with torch.no_grad():
    pretrained_output = model(test_grayscale).cpu()  #output from the pretrained model
    finetuned_output = model(test_grayscale).cpu()   #output after fine-tuning

display_comparison(test_grayscale.cpu(), test_rgb, pretrained_output, finetuned_output)

from skimage.metrics import structural_similarity as ssim

def calculate_ssim(image1, image2):
    image1_np = image1.permute(1, 2, 0).cpu().detach().numpy()
    image2_np = image2.permute(1, 2, 0).cpu().detach().numpy()
    return ssim(image1_np, image2_np, win_size=3, channel_axis=-1, data_range=1.0)

def evaluate_ssim(pre_output, post_output, ground_truth):
    ssim_pre = [calculate_ssim(pre, gt) for pre, gt in zip(pre_output, ground_truth)]
    ssim_post = [calculate_ssim(post, gt) for post, gt in zip(post_output, ground_truth)]
    avg_ssim_pre = sum(ssim_pre) / len(ssim_pre)
    avg_ssim_post = sum(ssim_post) / len(ssim_post)
    return avg_ssim_pre, avg_ssim_post

avg_ssim_pre, avg_ssim_post = evaluate_ssim(pre_finetune_output, finetuned_output, test_rgb)
print(f"Average SSIM before fine-tuning: {avg_ssim_pre:.4f}")
print(f"Average SSIM after fine-tuning: {avg_ssim_post:.4f}")

from skimage.color import rgb2lab
import numpy as np

def calculate_colorfulness(image):
    lab_image = rgb2lab(image.permute(1, 2, 0).cpu().detach().numpy())
    a_channel, b_channel = lab_image[:, :, 1], lab_image[:, :, 2]

    a_std, b_std = np.std(a_channel), np.std(b_channel)

    colorfulness_score = np.sqrt(a_std**2 + b_std**2)
    return colorfulness_score

avg_colorfulness_pre, avg_colorfulness_post = evaluate_colorfulness(pre_finetune_output, finetuned_output)
print(f"Average Colorfulness before fine-tuning: {avg_colorfulness_pre:.4f}")
print(f"Average Colorfulness after fine-tuning: {avg_colorfulness_post:.4f}")